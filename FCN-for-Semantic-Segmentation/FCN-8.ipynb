{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import copy, math\n",
    "import numpy as np\n",
    "from skimage.io import imread, imsave\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Deconvolution2D, Cropping2D\n",
    "from keras.layers import Input, Add, Dropout, Permute, add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create to a series of CONV layers followed by Max pooling layer\n",
    "def Convblock(channel_dimension, block_no, no_of_convs) :\n",
    "    Layers = []\n",
    "    for i in range(no_of_convs) :\n",
    "        \n",
    "        Conv_name = \"conv\"+str(block_no)+\"_\"+str(i+1)\n",
    "        \n",
    "        # A constant kernel size of 3*3 is used for all convolutions\n",
    "        Layers.append(Convolution2D(channel_dimension,kernel_size = (3,3),padding = \"same\",activation = \"relu\",name = Conv_name))\n",
    "    \n",
    "    Max_pooling_name = \"pool\"+str(block_no)\n",
    "    \n",
    "    #Addding max pooling layer\n",
    "    Layers.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),name = Max_pooling_name))\n",
    "    \n",
    "    return Layers\n",
    "\n",
    "def FCN_8_helper(image_size):\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = (image_size,image_size,3)))\n",
    "    \n",
    "    for l in Convblock(64,1,2) :\n",
    "        model.add(l)\n",
    "    \n",
    "    for l in Convblock(128,2,2):\n",
    "        model.add(l)\n",
    "    \n",
    "    for l in Convblock(256,3,3):\n",
    "        model.add(l)\n",
    "    \n",
    "    for l in Convblock(512,4,3):\n",
    "        model.add(l)\n",
    "    \n",
    "    for l in Convblock(512,5,3):\n",
    "        model.add(l)\n",
    "        \n",
    "    model.add(Convolution2D(4096,kernel_size=(7,7),padding = \"same\",activation = \"relu\",name = \"fc6\"))\n",
    "      \n",
    "    #Replacing fully connnected layers of VGG Net using convolutions\n",
    "    model.add(Convolution2D(4096,kernel_size=(1,1),padding = \"same\",activation = \"relu\",name = \"fc7\"))\n",
    "    \n",
    "    # Gives the classifications scores for each of the 21 classes including background\n",
    "    model.add(Convolution2D(21,kernel_size=(1,1),padding=\"same\",activation=\"relu\",name = \"score_fr\"))\n",
    "    \n",
    "    Conv_size = model.layers[-1].output_shape[2] #16 if image size if 512\n",
    "    #print(Conv_size)\n",
    "    \n",
    "    model.add(Deconvolution2D(21,kernel_size=(4,4),strides = (2,2),padding = \"valid\",activation=None,name = \"score2\"))\n",
    "    \n",
    "    # O = ((I-K+2*P)/Stride)+1 \n",
    "    # O = Output dimesnion after convolution\n",
    "    # I = Input dimnesion\n",
    "    # K = kernel Size\n",
    "    # P = Padding\n",
    "    \n",
    "    # I = (O-1)*Stride + K \n",
    "    Deconv_size = model.layers[-1].output_shape[2] #34 if image size is 512*512\n",
    "    \n",
    "    #print(Deconv_size)\n",
    "    # 2 if image size is 512*512\n",
    "    Extra = (Deconv_size - 2*Conv_size)\n",
    "    \n",
    "    #print(Extra)\n",
    "    \n",
    "    #Cropping to get correct size\n",
    "    model.add(Cropping2D(cropping=((0,Extra),(0,Extra))))\n",
    "    \n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN_8(image_size):\n",
    "    fcn_8 = FCN_8_helper(image_size)\n",
    "    #Calculating conv size after the sequential block\n",
    "    #32 if image size is 512*512\n",
    "    Conv_size = fcn_8.layers[-1].output_shape[2] \n",
    "    \n",
    "    #Conv to be applied on Pool4\n",
    "    skip_con1 = Convolution2D(21,kernel_size=(1,1),padding = \"same\",activation=None, name = \"score_pool4\")\n",
    "    \n",
    "    #Addig skip connection which takes adds the output of Max pooling layer 4 to current layer\n",
    "    Summed = add(inputs = [skip_con1(fcn_8.layers[14].output),fcn_8.layers[-1].output])\n",
    "    \n",
    "    #Upsampling output of first skip connection\n",
    "    x = Deconvolution2D(21,kernel_size=(4,4),strides = (2,2),padding = \"valid\",activation=None,name = \"score4\")(Summed)\n",
    "    x = Cropping2D(cropping=((0,2),(0,2)))(x)\n",
    "    \n",
    "    \n",
    "    #Conv to be applied to pool3\n",
    "    skip_con2 = Convolution2D(21,kernel_size=(1,1),padding = \"same\",activation=None, name = \"score_pool3\")\n",
    "    \n",
    "    #Adding skip connection which takes output og Max pooling layer 3 to current layer\n",
    "    Summed = add(inputs = [skip_con2(fcn_8.layers[10].output),x])\n",
    "    \n",
    "    #Final Up convolution which restores the original image size\n",
    "    Up = Deconvolution2D(21,kernel_size=(16,16),strides = (8,8),\n",
    "                         padding = \"valid\",activation = None,name = \"upsample\")(Summed)\n",
    "    \n",
    "    #Cropping the extra part obtained due to transpose convolution\n",
    "    final = Cropping2D(cropping = ((0,8),(0,8)))(Up)\n",
    "    \n",
    "    \n",
    "    return Model(fcn_8.input, final)\n",
    "\n",
    "model = FCN_8(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading weights from matlab file\n",
    "data = loadmat('pascal-fcn8s-dag.mat', matlab_compatible=False, struct_as_record=False)\n",
    "layers = data['layers']\n",
    "params = data['params']\n",
    "description = data['meta'][0,0].classes[0,0].description\n",
    "\n",
    "#Note : We are not transfering the weights of score4, score_pool3 and upsample because it doesn't follow the same \n",
    "# convention. The weights and biases for them are transferred below seperately\n",
    "\n",
    "def copy_mat_of_keras(kmodel):\n",
    "    kerasnames = [lr.name for lr in kmodel.layers]\n",
    "\n",
    "    prmt = (0, 1, 2, 3) # WARNING : important setting as 2 of the 4 axis have same size dimension\n",
    "    \n",
    "    for i in range(0, 35, 2):\n",
    "        matname = '_'.join(params[0,i].name[0].split('_')[0:-1])\n",
    "        if matname in kerasnames:\n",
    "            print(matname)\n",
    "            kindex = kerasnames.index(matname)\n",
    "            print('found : ', (str(matname), kindex))\n",
    "            l_weights = params[0,i].value\n",
    "            l_bias = params[0,i+1].value\n",
    "            f_l_weights = l_weights.transpose(prmt)\n",
    "            if False: # WARNING : this depends on \"image_data_format\":\"channels_last\" in keras.json file\n",
    "                f_l_weights = np.flip(f_l_weights, 0)\n",
    "                f_l_weights = np.flip(f_l_weights, 1)\n",
    "            print(f_l_weights.shape, kmodel.layers[kindex].get_weights()[0].shape)\n",
    "            assert (f_l_weights.shape == kmodel.layers[kindex].get_weights()[0].shape)\n",
    "            print(f_l_weights.shape)\n",
    "            print(\"layer\")\n",
    "            print(kmodel.layers[kindex].get_weights()[0].shape)\n",
    "            print(\"layer\")\n",
    "            assert (l_bias.shape[1] == 1)\n",
    "            print(l_bias[:,0].shape)\n",
    "            print(\"bias\")\n",
    "            print(kmodel.layers[kindex].get_weights()[1].shape)\n",
    "            print(\"bias\")\n",
    "            assert (l_bias[:,0].shape == kmodel.layers[kindex].get_weights()[1].shape)\n",
    "            assert (len(kmodel.layers[kindex].get_weights()) == 2)\n",
    "            kmodel.layers[kindex].set_weights([f_l_weights, l_bias[:,0]])\n",
    "        else:\n",
    "            print('not found : ', str(matname))\n",
    "    return kmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy_mat_of_keras(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasnames = [lr.name for lr in model.layers]\n",
    "\n",
    "#Getting the index of layer by name\n",
    "kindex = kerasnames.index('score4')\n",
    "l_weights = params[0,36].value\n",
    "bias = np.zeros(21)\n",
    "model.layers[27].set_weights([l_weights,bias])\n",
    "\n",
    "kindex = kerasnames.index('score_pool3')\n",
    "l_weights = params[0,37].value\n",
    "bias = params[0,38].value\n",
    "bias = bias[:,0]\n",
    "model.layers[28].set_weights([l_weights,bias])\n",
    "\n",
    "kindex = kerasnames.index('upsample')\n",
    "lweights = params[0,39].value\n",
    "bias = np.zeros(21)\n",
    "model.layers[31].set_weights([lweights,bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('TestImages/2007_000033.jpg') \n",
    "im = im.crop((0,0,319,319)) \n",
    "im = im.resize((512,512))\n",
    "\n",
    "plt.imshow(np.asarray(im))\n",
    "imsave(\"orginal_1.png\",im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(kmodel, crpimg, transform=False):\n",
    "    \n",
    "    imarr = np.array(crpimg).astype(np.float32)\n",
    "\n",
    "    if transform:\n",
    "        imarr[:, :, 0] -= 129.1863\n",
    "        imarr[:, :, 1] -= 104.7624\n",
    "        imarr[:, :, 2] -= 93.5940\n",
    "\n",
    "        aux = copy.copy(imarr)\n",
    "        imarr[:, :, 0] = aux[:, :, 2]\n",
    "        imarr[:, :, 2] = aux[:, :, 0]\n",
    "\n",
    "    imarr = np.expand_dims(imarr, axis=0)\n",
    "\n",
    "    return kmodel.predict(imarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crpim = im \n",
    "preds = prediction(model, crpim, transform=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imclass = np.argmax(preds, axis=3)[0,:,:]\n",
    "imsave(\"annotation1_fcn8.png\",imclass)\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow( np.asarray(crpim) )\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow( imclass )\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow( np.asarray(crpim) )\n",
    "masked_imclass = np.ma.masked_where(imclass == 0, imclass)\n",
    "plt.imshow( masked_imclass, alpha=0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dominant classes found in the image\n",
    "for c in np.unique(imclass):\n",
    "    print(c, str(description[0,c][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
